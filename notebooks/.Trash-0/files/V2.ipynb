{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21807fd-3000-4f8c-86e1-c05de6c84000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python Path: ['/app', '/usr/local/lib/python39.zip', '/usr/local/lib/python3.9', '/usr/local/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the directory where your notebook is running (which is /app/notebooks)\n",
    "notebook_dir = os.path.dirname(os.path.abspath(''))\n",
    "\n",
    "# Add the parent directory (/app) to the Python path\n",
    "# This allows imports like 'from etl_development.my_etl_module import ...'\n",
    "if '/app' not in sys.path:\n",
    "    sys.path.insert(0, '/app') # Add /app to the beginning of the path for highest priority\n",
    "\n",
    "print(f\"Current Python Path: {sys.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80be39",
   "metadata": {},
   "source": [
    "# 01 - Acquire Packages For Data Preparation Work\n",
    "Includes:\n",
    " - loading required PACKAGES\n",
    " - provided DIRECTORY for filepaths and github/version control functionality for structuring data and frameworks (not fully used as had limited time)\n",
    " - FUNCTIONS (which would have been encapsulated within classes and appropriate github/version control directories - but with limited time only included here).\n",
    " - A data type/preparation function (TRANSFORMATION - fn_transform_cast)\n",
    " - A function which creates dimensional tables (TRANSFORMATION for Star Schemas - fn_create_dim_table)\n",
    " - A function that creates spark dataframes based on user suggestions (EXTRACTION/LOAD - fn_dataframe_selections)\n",
    " - A function that extends/creates attributes relevant to dates (TRANSFORMATION - fn_add_period_attributes)\n",
    " - Functions include testing, error checking, correction, and validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c1a0a76-af1e-4e8a-a619-b46dd3b9ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType\n",
    "from pyspark.sql.functions import col, to_date, year, month, dayofmonth, hour, month, date_format, count\n",
    "import shutil # Integration with version control platforms (github, gitlab, etc.)\n",
    "import pandas as pd # Import pandas\n",
    "import datetime\n",
    "from etl_development.joels_etl_class import JoelsETL\n",
    "\n",
    "# Pathways to (1) data, \n",
    "# (2) output data quality/profiling used to examine and determine transformation requirements, \n",
    "# (3) star schema output (warehouse)\n",
    "fpath_tenancy_data = '../src_data/sample.json'\n",
    "fpath_etl_code = '/app/etl_development/'\n",
    "fpath_data_quality_profile = '/app/data_quality_profiles/'\n",
    "fpath_data_star_schema = '../data_star_schema_prep/ais_data_by_multi_periods'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "755815fe-31d6-4b8f-bdb9-e3c6ae143be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 14:33:31 WARN TaskSetManager: Stage 62 contains a task of very large size (36174 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data partitioned by year/month/day/hour\n"
     ]
    }
   ],
   "source": [
    "#Test 5\n",
    "\n",
    "#df.show(10, truncate=False)\n",
    "#df.printSchema()\n",
    "\n",
    "#df.describe()\n",
    "#df.select(\"DateTime\").(\"min\", \"25%\", \"75%\", \"max\").show()\n",
    "#df2 = df.withColumn(\"Date_Only\",to_date(\"DateTime\"))\n",
    "#df.select(\"Date_Only\", \").(\"min\", \"25%\", \"75%\", \"max\").show()\n",
    "\n",
    "# Add hierarchical partition columns\n",
    "df_with_hour = df.withColumn(\"year\", year(col(\"DateTime\"))) \\\n",
    "                 .withColumn(\"month\", month(col(\"DateTime\"))) \\\n",
    "                 .withColumn(\"day\", dayofmonth(col(\"DateTime\"))) \\\n",
    "                 .withColumn(\"hour\", hour(col(\"DateTime\")))\n",
    "\n",
    "# Save with hierarchical partitioning\n",
    "df_with_hour.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\", \"hour\") \\\n",
    "    .parquet(fpath_data_star_schema)\n",
    "\n",
    "print(\"Data partitioned by year/month/day/hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76850bfb-b4ed-40f2-822f-ddefbbfa8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 10:30:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/07/28 10:30:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with typed schema in chunks of ~2048MB...\n",
      "  Read 100,000 lines...\n",
      "  Read 200,000 lines...\n",
      "  Read 300,000 lines...\n",
      "  Read 400,000 lines...\n",
      "  Read 500,000 lines...\n",
      "  Read 600,000 lines...\n",
      "  Read 700,000 lines...\n",
      "  Read 800,000 lines...\n",
      "  Read 900,000 lines...\n",
      "  Read 1,000,000 lines...\n",
      "  Read 1,100,000 lines...\n",
      "  Read 1,200,000 lines...\n",
      "  Read 1,300,000 lines...\n",
      "  Read 1,400,000 lines...\n",
      "  Read 1,500,000 lines...\n",
      "  Read 1,600,000 lines...\n",
      "  Read 1,700,000 lines...\n",
      "  Read 1,800,000 lines...\n",
      "  Read 1,900,000 lines...\n",
      "  Read 2,000,000 lines...\n",
      "  Read 2,100,000 lines...\n",
      "  Read 2,200,000 lines...\n",
      "  Read 2,300,000 lines...\n",
      "  Read 2,400,000 lines...\n",
      "  Read 2,500,000 lines...\n",
      "  Read 2,600,000 lines...\n",
      "  Read 2,700,000 lines...\n",
      "  Read 2,800,000 lines...\n",
      "  Read 2,900,000 lines...\n",
      "  Read 3,000,000 lines...\n",
      "  Read 3,100,000 lines...\n",
      "  Read 3,200,000 lines...\n",
      "  Read 3,300,000 lines...\n",
      "  Read 3,400,000 lines...\n",
      "  Read 3,500,000 lines...\n",
      "  Read 3,600,000 lines...\n",
      "  Read 3,700,000 lines...\n",
      "  Read 3,800,000 lines...\n",
      "  Read 3,900,000 lines...\n",
      "  Read 4,000,000 lines...\n",
      "  Read 4,100,000 lines...\n",
      "  Read 4,200,000 lines...\n",
      "  Read 4,300,000 lines...\n",
      "  Read 4,400,000 lines...\n",
      "  Read 4,500,000 lines...\n",
      "  Read 4,600,000 lines...\n",
      "  Read 4,700,000 lines...\n",
      "  Read 4,800,000 lines...\n",
      "  Read 4,900,000 lines...\n",
      "  Read 5,000,000 lines...\n",
      "  Read 5,100,000 lines...\n",
      "  Read 5,200,000 lines...\n",
      "  Read 5,300,000 lines...\n",
      "  Read 5,400,000 lines...\n",
      "  Read 5,500,000 lines...\n",
      "  Read 5,600,000 lines...\n",
      "  Read 5,700,000 lines...\n",
      "  Read 5,800,000 lines...\n",
      "  Read 5,900,000 lines...\n",
      "  Read 6,000,000 lines...\n",
      "  Read 6,100,000 lines...\n",
      "  Read 6,200,000 lines...\n",
      "  Read 6,300,000 lines...\n",
      "  Read 6,400,000 lines...\n",
      "  Read 6,500,000 lines...\n",
      "  Read 6,600,000 lines...\n",
      "  Read 6,700,000 lines...\n",
      "  Read 6,800,000 lines...\n",
      "  Read 6,900,000 lines...\n",
      "  Read 7,000,000 lines...\n",
      "  Read 7,100,000 lines...\n",
      "  Read 7,200,000 lines...\n",
      "  Read 7,300,000 lines...\n",
      "  Read 7,400,000 lines...\n",
      "  Read 7,500,000 lines...\n",
      "  Read 7,600,000 lines...\n",
      "  Read 7,700,000 lines...\n",
      "  Read 7,800,000 lines...\n",
      "  Read 7,900,000 lines...\n",
      "  Read 8,000,000 lines...\n",
      "  Read 8,100,000 lines...\n",
      "  Read 8,200,000 lines...\n",
      "  Read 8,300,000 lines...\n",
      "  Read 8,400,000 lines...\n",
      "  Read 8,500,000 lines...\n",
      "  Read 8,600,000 lines...\n",
      "  Read 8,700,000 lines...\n",
      "  Read 8,800,000 lines...\n",
      "  Read 8,900,000 lines...\n",
      "Processing final chunk 1 with 8,955,836 records...\n",
      "Total processed: 8,955,836 records in 1 chunks\n",
      "Combining chunks and applying final transformations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 10:54:56 WARN TaskSetManager: Stage 5 contains a task of very large size (36174 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame created with 8,955,836 records\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- DateTime: timestamp (nullable = true)\n",
      " |-- UTCTimeStamp: integer (nullable = true)\n",
      " |-- Message_MessageID: integer (nullable = true)\n",
      " |-- Message_UserID: integer (nullable = true)\n",
      " |-- Message_Latitude: double (nullable = true)\n",
      " |-- Message_Longitude: double (nullable = true)\n",
      " |-- Message_SOG: double (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 10:55:29 WARN TaskSetManager: Stage 8 contains a task of very large size (36174 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/28 10:55:33 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 8 (TID 43): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----------------+--------------+----------------+-----------------+-----------+\n",
      "|DateTime           |UTCTimeStamp|Message_MessageID|Message_UserID|Message_Latitude|Message_Longitude|Message_SOG|\n",
      "+-------------------+------------+-----------------+--------------+----------------+-----------------+-----------+\n",
      "|2020-05-05 00:00:00|1588636800  |18               |416004341     |-7.578556       |171.328116       |NULL       |\n",
      "|2020-05-05 00:00:00|1588636800  |18               |503648300     |-22.647916      |152.895326       |NULL       |\n",
      "|2020-05-05 00:00:00|1588636800  |18               |503671200     |-22.527651      |152.926453       |NULL       |\n",
      "|2020-05-05 00:00:00|1588636800  |18               |512235000     |-40.675575      |173.762283       |NULL       |\n",
      "|2020-05-05 00:00:00|1588636800  |18               |601224000     |-33.024055      |17.954858        |NULL       |\n",
      "|2020-05-05 00:00:00|1588636800  |18               |664060000     |-34.78765       |70.76879         |NULL       |\n",
      "|2020-05-05 00:00:00|1588636800  |19               |398701904     |-31.747185      |164.280313       |NULL       |\n",
      "|2020-05-05 00:00:00|1588636800  |19               |58015872      |-7.88115        |-167.767201      |NULL       |\n",
      "|2020-05-05 00:00:00|1588636800  |19               |98511881      |-5.960218       |75.884096        |NULL       |\n",
      "|2020-05-05 00:00:00|1588636800  |1                |209761000     |-23.2088        |8.603116         |NULL       |\n",
      "+-------------------+------------+-----------------+--------------+----------------+-----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Data type verification:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 10:55:34 WARN TaskSetManager: Stage 9 contains a task of very large size (36174 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/28 10:55:38 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 9 (TID 44): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime type: <class 'datetime.datetime'>\n",
      "Latitude: -7.578556 (type: <class 'float'>)\n",
      "Longitude: 171.328116 (type: <class 'float'>)\n",
      "MessageID: 18 (type: <class 'int'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 10:55:39 WARN TaskSetManager: Stage 10 contains a task of very large size (36174 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/28 10:56:26 WARN TaskSetManager: Stage 11 contains a task of very large size (36174 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 11:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame cached with 8,955,836 total records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Test 4\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from decimal import Decimal, ROUND_DOWN\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import col, from_unixtime, round as spark_round\n",
    "\n",
    "def process_with_typed_schema(file_path, max_memory_mb=2048):\n",
    "    \"\"\"Process with proper data types and conversions\"\"\"\n",
    "    \n",
    "    # Define schema with proper types\n",
    "    schema = StructType([\n",
    "        StructField(\"UTCTimeStamp\", IntegerType(), True),  # Will convert to timestamp later\n",
    "        StructField(\"Message_MessageID\", IntegerType(), True),\n",
    "        StructField(\"Message_UserID\", IntegerType(), True),\n",
    "        StructField(\"Message_Latitude\", DoubleType(), True),\n",
    "        StructField(\"Message_Longitude\", DoubleType(), True),\n",
    "        StructField(\"Message_SOG\", DoubleType(), True)\n",
    "        # Add more numeric fields as needed\n",
    "    ])\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"TypedProcessor\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    def truncate_decimal(value, decimal_places=6):\n",
    "        \"\"\"Truncate decimal to specified places\"\"\"\n",
    "        if value is None:\n",
    "            return None\n",
    "        try:\n",
    "            # Convert to Decimal for precise truncation\n",
    "            decimal_val = Decimal(str(value))\n",
    "            multiplier = Decimal(10 ** decimal_places)\n",
    "            return float(decimal_val.quantize(Decimal('1') / multiplier, rounding=ROUND_DOWN))\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def safe_convert_int(value):\n",
    "        \"\"\"Safely convert to integer\"\"\"\n",
    "        if value is None or value == '':\n",
    "            return None\n",
    "        try:\n",
    "            return int(float(value))  # Handle strings like \"123.0\"\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def safe_convert_float(value):\n",
    "        \"\"\"Safely convert to float\"\"\"\n",
    "        if value is None or value == '':\n",
    "            return None\n",
    "        try:\n",
    "            return float(value)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    max_memory_bytes = max_memory_mb * 1024 * 1024\n",
    "    chunk_records = []\n",
    "    current_memory = 0\n",
    "    all_dataframes = []\n",
    "    chunk_number = 0\n",
    "    total_processed = 0\n",
    "    \n",
    "    print(f\"Processing with typed schema in chunks of ~{max_memory_mb}MB...\")\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                message = record.get('Message', {})\n",
    "                \n",
    "                # Create properly typed record\n",
    "                flat_record = {\n",
    "                    'UTCTimeStamp': safe_convert_int(record.get('UTCTimeStamp')),\n",
    "                    'Message_MessageID': safe_convert_int(message.get('MessageID')),\n",
    "                    'Message_UserID': safe_convert_int(message.get('UserID')),\n",
    "                    'Message_Latitude': truncate_decimal(safe_convert_float(message.get('Latitude')), 6),\n",
    "                    'Message_Longitude': truncate_decimal(safe_convert_float(message.get('Longitude')), 6),\n",
    "                    'Message_SOG': safe_convert_float(message.get('SOG'))\n",
    "                }\n",
    "                \n",
    "                chunk_records.append(flat_record)\n",
    "                current_memory += sys.getsizeof(str(flat_record))\n",
    "                \n",
    "                if current_memory >= max_memory_bytes:\n",
    "                    chunk_number += 1\n",
    "                    print(f\"Processing chunk {chunk_number} with {len(chunk_records):,} records...\")\n",
    "                    \n",
    "                    # Create DataFrame with typed schema\n",
    "                    chunk_df = spark.createDataFrame(chunk_records, schema)\n",
    "                    all_dataframes.append(chunk_df)\n",
    "                    \n",
    "                    total_processed += len(chunk_records)\n",
    "                    print(f\"  Chunk {chunk_number} processed. Total so far: {total_processed:,}\")\n",
    "                    \n",
    "                    chunk_records = []\n",
    "                    current_memory = 0\n",
    "                \n",
    "                if line_num % 100000 == 0:\n",
    "                    print(f\"  Read {line_num:,} lines...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if line_num <= 10:  # Show first 10 errors only\n",
    "                    print(f\"Error at line {line_num}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Process final chunk\n",
    "    if chunk_records:\n",
    "        chunk_number += 1\n",
    "        print(f\"Processing final chunk {chunk_number} with {len(chunk_records):,} records...\")\n",
    "        chunk_df = spark.createDataFrame(chunk_records, schema)\n",
    "        all_dataframes.append(chunk_df)\n",
    "        total_processed += len(chunk_records)\n",
    "    \n",
    "    print(f\"Total processed: {total_processed:,} records in {chunk_number} chunks\")\n",
    "    \n",
    "    # Union all chunks\n",
    "    if all_dataframes:\n",
    "        print(\"Combining chunks and applying final transformations...\")\n",
    "        final_df = all_dataframes[0]\n",
    "        for df in all_dataframes[1:]:\n",
    "            final_df = final_df.union(df)\n",
    "        \n",
    "        # Convert UTCTimeStamp to datetime and ensure lat/lon precision\n",
    "        final_df = final_df.withColumn(\n",
    "            \"DateTime\", \n",
    "            from_unixtime(col(\"UTCTimeStamp\")).cast(TimestampType())\n",
    "        ).withColumn(\n",
    "            \"Message_Latitude\", \n",
    "            spark_round(col(\"Message_Latitude\"), 6)\n",
    "        ).withColumn(\n",
    "            \"Message_Longitude\", \n",
    "            spark_round(col(\"Message_Longitude\"), 6)\n",
    "        )\n",
    "        \n",
    "        # Reorder columns to put DateTime first\n",
    "        column_order = [\"DateTime\", \"UTCTimeStamp\"] + [c for c in final_df.columns if c not in [\"DateTime\", \"UTCTimeStamp\"]]\n",
    "        final_df = final_df.select(*column_order)\n",
    "        \n",
    "        print(f\"Final DataFrame created with {final_df.count():,} records\")\n",
    "        return final_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Usage\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TypedJSONProcessor\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Process your file\n",
    "df = process_with_typed_schema(\"../src_data/sample.json\", max_memory_mb=2048)\n",
    "\n",
    "if df:\n",
    "    print(\"\\nSchema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(\"\\nSample data:\")\n",
    "    df.show(10, truncate=False)\n",
    "    \n",
    "    # Show data type verification\n",
    "    print(\"\\nData type verification:\")\n",
    "    sample_row = df.first()\n",
    "    if sample_row:\n",
    "        print(f\"DateTime type: {type(sample_row['DateTime'])}\")\n",
    "        print(f\"Latitude: {sample_row['Message_Latitude']} (type: {type(sample_row['Message_Latitude'])})\")\n",
    "        print(f\"Longitude: {sample_row['Message_Longitude']} (type: {type(sample_row['Message_Longitude'])})\")\n",
    "        print(f\"MessageID: {sample_row['Message_MessageID']} (type: {type(sample_row['Message_MessageID'])})\")\n",
    "    \n",
    "    # Cache for reuse\n",
    "    df.cache()\n",
    "    print(f\"\\nDataFrame cached with {df.count():,} total records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9df1b9d-1b08-47b5-9f32-159960311a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected schema:\n",
      "  UTCTimeStamp: IntegerType()\n",
      "  Message_MessageID: IntegerType()\n",
      "  Message_RepeatIndicator: IntegerType()\n",
      "  Message_UserID: IntegerType()\n",
      "  Message_Valid: IntegerType()\n",
      "  Message_Spare1: IntegerType()\n",
      "  Message_Sog: DoubleType()\n",
      "  Message_PositionAccuracy: IntegerType()\n",
      "  Message_Longitude: DoubleType()\n",
      "  Message_Latitude: DoubleType()\n",
      "  Message_Cog: DoubleType()\n",
      "  Message_TrueHeading: IntegerType()\n",
      "  Message_Timestamp: IntegerType()\n",
      "  Message_Spare2: IntegerType()\n",
      "  Message_ClassBUnit: IntegerType()\n",
      "  Message_ClassBDisplay: IntegerType()\n",
      "  Message_ClassBDsc: IntegerType()\n",
      "  Message_ClassBBand: IntegerType()\n",
      "  Message_ClassBMsg22: IntegerType()\n",
      "  Message_AssignedMode: IntegerType()\n",
      "  Message_Raim: IntegerType()\n",
      "  Message_CommunicationStateIsItdma: IntegerType()\n",
      "  Message_CommunicationState: IntegerType()\n",
      "  Message_Name: StringType()\n",
      "  Message_Type: IntegerType()\n",
      "  Message_Dimension: StringType()\n",
      "  Message_FixType: IntegerType()\n",
      "  Message_Dte: IntegerType()\n",
      "  Message_Spare3: IntegerType()\n",
      "  Message_NavigationalStatus: IntegerType()\n",
      "  Message_RateOfTurn: IntegerType()\n",
      "  Message_SpecialManoeuvreIndicator: IntegerType()\n",
      "  Message_Spare: IntegerType()\n",
      "  Message_Reserved: IntegerType()\n",
      "  Message_PartNumber: IntegerType()\n",
      "  Message_ReportA: StringType()\n",
      "  Message_ReportB: StringType()\n",
      "  Message_PositionLatency: IntegerType()\n",
      "  Message_AisVersion: IntegerType()\n",
      "  Message_ImoNumber: IntegerType()\n",
      "  Message_CallSign: StringType()\n",
      "  Message_Eta: StringType()\n",
      "  Message_MaximumStaticDraught: DoubleType()\n",
      "  Message_Destination: StringType()\n",
      "  Message_UtcYear: IntegerType()\n",
      "  Message_UtcMonth: IntegerType()\n",
      "  Message_UtcDay: IntegerType()\n",
      "  Message_UtcHour: IntegerType()\n",
      "  Message_UtcMinute: IntegerType()\n",
      "  Message_UtcSecond: IntegerType()\n",
      "  Message_LongRangeEnable: IntegerType()\n",
      "  Message_ApplicationID: StringType()\n",
      "  Message_BinaryData: StringType()\n",
      "  Message_DestinationIDValid: IntegerType()\n",
      "  Message_ApplicationIDValid: IntegerType()\n",
      "  Message_DestinationID: IntegerType()\n",
      "  Message_Payload: StringType()\n"
     ]
    }
   ],
   "source": [
    "#Test 3\n",
    "def get_dynamic_schema_with_types(file_path, sample_lines=1000):\n",
    "    \"\"\"Analyze first N lines to build typed schema dynamically\"\"\"\n",
    "    \n",
    "    field_samples = {}\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= sample_lines:\n",
    "                break\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                message = record.get('Message', {})\n",
    "                \n",
    "                # Collect all field samples\n",
    "                for key, value in record.items():\n",
    "                    if key != 'Message':\n",
    "                        field_samples.setdefault(key, []).append(value)\n",
    "                \n",
    "                for key, value in message.items():\n",
    "                    field_name = f\"Message_{key}\"\n",
    "                    field_samples.setdefault(field_name, []).append(value)\n",
    "                    \n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Build schema based on samples\n",
    "    schema_fields = []\n",
    "    \n",
    "    for field_name, samples in field_samples.items():\n",
    "        non_null_samples = [s for s in samples if s is not None]\n",
    "        \n",
    "        if field_name == 'UTCTimeStamp':\n",
    "            schema_fields.append(StructField(field_name, IntegerType(), True))\n",
    "        elif 'Latitude' in field_name or 'Longitude' in field_name:\n",
    "            schema_fields.append(StructField(field_name, DoubleType(), True))\n",
    "        elif any(isinstance(s, (int, float)) for s in non_null_samples):\n",
    "            # Check if it's integer or float\n",
    "            if all(isinstance(s, int) or (isinstance(s, float) and s.is_integer()) \n",
    "                   for s in non_null_samples if isinstance(s, (int, float))):\n",
    "                schema_fields.append(StructField(field_name, IntegerType(), True))\n",
    "            else:\n",
    "                schema_fields.append(StructField(field_name, DoubleType(), True))\n",
    "        else:\n",
    "            schema_fields.append(StructField(field_name, StringType(), True))\n",
    "    \n",
    "    return StructType(schema_fields), field_samples.keys()\n",
    "\n",
    "# Use dynamic schema detection\n",
    "schema, field_names = get_dynamic_schema_with_types(\"../src_data/sample.json\", 1000)\n",
    "print(\"Detected schema:\")\n",
    "for field in schema.fields:\n",
    "    print(f\"  {field.name}: {field.dataType}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f800aa9-be28-463b-a2f7-e0ca96d094bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid JSON records: 8955836\n",
      "Invalid JSON lines: 0\n",
      "Total lines: 8955836\n"
     ]
    }
   ],
   "source": [
    "#Test 2\n",
    "\n",
    "import json\n",
    "\n",
    "def count_valid_json_records(file_path):\n",
    "    \"\"\"Count valid JSON records and track errors\"\"\"\n",
    "    valid_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                json.loads(line.strip())\n",
    "                valid_count += 1\n",
    "            except json.JSONDecodeError:\n",
    "                error_count += 1\n",
    "                if error_count <= 5:  # Show first 5 errors\n",
    "                    print(f\"Invalid JSON at line {i+1}: {line[:100]}...\")\n",
    "    \n",
    "    print(f\"Valid JSON records: {valid_count}\")\n",
    "    print(f\"Invalid JSON lines: {error_count}\")\n",
    "    print(f\"Total lines: {valid_count + error_count}\")\n",
    "    \n",
    "    return valid_count, error_count\n",
    "\n",
    "# Count valid records\n",
    "valid, invalid = count_valid_json_records(\"../src_data/sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "585973cd-d2cf-4b55-9228-f94d60b4e059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 10:00:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 100 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+---------+-------------------+-------------------+----+\n",
      "|UTCTimeStamp|MessageID|UserID   |Latitude           |Longitude          |SOG |\n",
      "+------------+---------+---------+-------------------+-------------------+----+\n",
      "|1588636800  |18       |416004341|-7.578556666666667 |171.32811666666666 |NULL|\n",
      "|1588636800  |18       |503648300|-22.647916666666667|152.89532666666668 |NULL|\n",
      "|1588636800  |18       |503671200|-22.527651666666667|152.92645333333334 |NULL|\n",
      "|1588636800  |18       |512235000|-40.675575         |173.76228333333333 |NULL|\n",
      "|1588636800  |18       |601224000|-33.024055         |17.954858333333334 |NULL|\n",
      "|1588636800  |18       |664060000|-34.78765          |70.76879000000001  |NULL|\n",
      "|1588636800  |19       |398701904|-31.747185         |164.2803133333333  |NULL|\n",
      "|1588636800  |19       |58015872 |-7.881150000000001 |-167.76720166666667|NULL|\n",
      "|1588636800  |19       |98511881 |-5.960218333333333 |75.88409666666666  |NULL|\n",
      "|1588636800  |1        |209761000|-23.2088           |8.603116666666667  |NULL|\n",
      "+------------+---------+---------+-------------------+-------------------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- UTCTimeStamp: string (nullable = true)\n",
      " |-- MessageID: string (nullable = true)\n",
      " |-- UserID: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- SOG: string (nullable = true)\n",
      "\n",
      "Total records in DataFrame: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TEST 1\n",
    "\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "def read_and_flatten_json_lines(file_path, num_lines=100):\n",
    "    \"\"\"Read first N lines and flatten the nested structure\"\"\"\n",
    "    flattened_records = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= num_lines:\n",
    "                break\n",
    "            try:\n",
    "                # Parse the JSON line\n",
    "                record = json.loads(line.strip())\n",
    "                \n",
    "                # Flatten the structure\n",
    "                flat_record = {\n",
    "                    'UTCTimeStamp': record.get('UTCTimeStamp'),\n",
    "                    'MessageID': record.get('Message', {}).get('MessageID'),\n",
    "                    'UserID': record.get('Message', {}).get('UserID'),\n",
    "                    'Latitude': record.get('Message', {}).get('Latitude'),\n",
    "                    'Longitude': record.get('Message', {}).get('Longitude'),\n",
    "                    'SOG': record.get('Message', {}).get('SOG')\n",
    "                }\n",
    "                \n",
    "                flattened_records.append(flat_record)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON at line {i+1}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {i+1}: {e}\")\n",
    "    \n",
    "    return flattened_records\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlattenedAIS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read and flatten first 100 lines\n",
    "flattened_data = read_and_flatten_json_lines(\"../src_data/sample.json\", 100)\n",
    "\n",
    "print(f\"Successfully processed {len(flattened_data)} records\")\n",
    "\n",
    "# Define schema for the flattened data\n",
    "schema = StructType([\n",
    "    StructField(\"UTCTimeStamp\", StringType(), True),\n",
    "    StructField(\"MessageID\", StringType(), True),\n",
    "    StructField(\"UserID\", StringType(), True),\n",
    "    StructField(\"Latitude\", StringType(), True),\n",
    "    StructField(\"Longitude\", StringType(), True),\n",
    "    StructField(\"SOG\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame from the flattened data\n",
    "df = spark.createDataFrame(flattened_data, schema)\n",
    "\n",
    "# Show the results\n",
    "df.show(10, truncate=False)\n",
    "df.printSchema()\n",
    "print(f\"Total records in DataFrame: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a8ddbd-ad7b-4459-bf31-1350a734b85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- active: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-----------+--------------------+----------+---+--------------------+-------------+------+--------------------+\n",
      "|active|age| department|               email| hire_date| id|            location|         name|salary|              skills|\n",
      "+------+---+-----------+--------------------+----------+---+--------------------+-------------+------+--------------------+\n",
      "|  true| 28|Engineering|john.smith@email.com|2022-03-15|001| {New York, USA, NY}|   John Smith| 75000|[Python, SQL, Doc...|\n",
      "|  true| 32|  Marketing|sarah.johnson@ema...|2021-07-22|002|{San Francisco, U...|Sarah Johnson| 68000|[Analytics, SEO, ...|\n",
      "| false| 26|     Design| mike.chen@email.com|2023-01-10|003|   {Austin, USA, TX}|    Mike Chen| 62000|[UI/UX, Figma, Pr...|\n",
      "|  true| 29|Engineering|emma.wilson@email...|2020-11-05|004|  {Seattle, USA, WA}|  Emma Wilson| 82000|[Java, Kubernetes...|\n",
      "|  true| 35|      Sales|david.brown@email...|2019-09-18|005|  {Chicago, USA, IL}|  David Brown| 71000|[CRM, Negotiation...|\n",
      "|  true| 31|         HR|lisa.garcia@email...|2022-06-12|006|    {Miami, USA, FL}|  Lisa Garcia| 65000|[Recruiting, Trai...|\n",
      "| false| 27|    Finance|alex.taylor@email...|2023-02-28|007|   {Boston, USA, MA}|  Alex Taylor| 69000|[Excel, Financial...|\n",
      "|  true| 30|Engineering|rachel.kim@email.com|2021-04-07|008| {Portland, USA, OR}|   Rachel Kim| 78000|[React, Node.js, ...|\n",
      "+------+---+-----------+--------------------+----------+---+--------------------+-------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadJSONSample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Method 1: Read all data then limit (simple but reads entire file)\n",
    "df = spark.read \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"samplingRatio\", 0.1) \\\n",
    "    .json(fpath_tenancy_data) \\\n",
    "    .limit(10)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "419b110f-523d-4d8c-970a-190ccb602729",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e70e49-5162-48e5-8716-c91b7962f1a6",
   "metadata": {},
   "source": [
    "# 02 - Setup Spark to ingest and ETL non-geospatial data\n",
    "Includes:\n",
    "- creating a spark session\n",
    "- accessing tenancy data from original file with ...\n",
    "- a pre-formed schema (non-parsing) to EXTRACT data (as is)\n",
    "- basic TRANSFORMATION/PREPARATION of non-geospatial datasets\n",
    "- examination of data quality leveraging YData Quality Framework tool (and saving to \\\"data quality profile\\\" folder in interactive html format.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c637d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/27 16:34:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read data from local file: /app/src_data/\n",
      "DataFrame head:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+------------+------------+------------+-----------+-------------------+-------------------+-------------------+-----------------------+\n",
      "|Time Frame|Location Id|            Location|Lodged Bonds|Active Bonds|Closed Bonds|Median Rent|Geometric Mean Rent|Upper Quartile Rent|Lower Quartile Rent|Log Std Dev Weekly Rent|\n",
      "+----------+-----------+--------------------+------------+------------+------------+-----------+-------------------+-------------------+-------------------+-----------------------+\n",
      "|1993-02-01|        -99|                 ALL|       9,144|       91635|       7,119|        150|                151|                200|                120|                   0.43|\n",
      "|1993-02-01|         -1|                  NA|         390|        4932|         336|        130|                128|                165|                100|                   0.46|\n",
      "|1993-02-01|          1|    Northland Region|         177|        1770|         141|        120|                119|                146|                100|                   0.30|\n",
      "|1993-02-01|          2|     Auckland Region|       2,379|       29037|       1,965|        180|                186|                220|                150|                   0.36|\n",
      "|1993-02-01|          3|      Waikato Region|         906|        8304|         693|        120|                125|                160|                100|                   0.34|\n",
      "|1993-02-01|          4|Bay of Plenty Region|         435|        4851|         369|        140|                135|                160|                120|                   0.30|\n",
      "|1993-02-01|          5|     Gisborne Region|          87|         780|          63|        120|                120|                140|                109|                   0.24|\n",
      "|1993-02-01|          6|  Hawke's Bay Region|         237|        2967|         225|        140|                133|                155|                120|                   0.26|\n",
      "|1993-02-01|          7|     Taranaki Region|         240|        2313|         183|        130|                128|                158|                108|                   0.36|\n",
      "|1993-02-01|          8|Manawatu-Wanganui...|         807|        5484|         552|        140|                142|                200|                110|                   0.47|\n",
      "|1993-02-01|          9|   Wellington Region|         975|       11376|         828|        170|                171|                213|                130|                   0.38|\n",
      "|1993-02-01|         12|   West Coast Region|          24|         399|          24|        120|                 99|                130|                 80|                   0.37|\n",
      "|1993-02-01|         13|   Canterbury Region|       1,242|       10731|         873|        150|                150|                190|                120|                   0.39|\n",
      "|1993-02-01|         14|        Otago Region|         897|        5058|         567|        150|                149|                220|                120|                   0.56|\n",
      "|1993-02-01|         15|    Southland Region|         150|        1434|          99|        100|                 92|                115|                 83|                   0.32|\n",
      "|1993-02-01|         16|       Tasman Region|          33|         309|          30|        140|                125|                166|                110|                   0.43|\n",
      "|1993-02-01|         17|       Nelson Region|         117|        1230|         117|        150|                145|                180|                129|                   0.26|\n",
      "|1993-02-01|         18|  Marlborough Region|          51|         654|          54|        130|                123|                150|                110|                   0.20|\n",
      "|1993-03-01|        -99|                 ALL|       7,812|       92724|       6,720|        150|                148|                180|                120|                   0.38|\n",
      "|1993-03-01|         -1|                  NA|         360|        4932|         360|        130|                125|                160|                100|                   0.44|\n",
      "+----------+-----------+--------------------+------------+------------+------------+-----------+-------------------+-------------------+-------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame schema:\n",
      "root\n",
      " |-- Time Frame: string (nullable = true)\n",
      " |-- Location Id: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Lodged Bonds: string (nullable = true)\n",
      " |-- Active Bonds: string (nullable = true)\n",
      " |-- Closed Bonds: string (nullable = true)\n",
      " |-- Median Rent: string (nullable = true)\n",
      " |-- Geometric Mean Rent: string (nullable = true)\n",
      " |-- Upper Quartile Rent: string (nullable = true)\n",
      " |-- Lower Quartile Rent: string (nullable = true)\n",
      " |-- Log Std Dev Weekly Rent: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadCSVLocal\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "etl_processor = JoelsETL(spark)\n",
    "# Define the schema explicitly\n",
    "# All columns are declared as StringType to load all and minimise parsing errors\n",
    "# due to mixed types or special values like \"-1\" or \"NA\" within quoted fields.\n",
    "# Casting occurs later\n",
    "\"\"\"\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\")  # Set to \"true\" if your CSV has a header row\n",
    "    .option(\"inferSchema\", \"false\") # Crucial: This makes all columns StringType\n",
    "    .csv(csv_file_path)\n",
    "\"\"\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Time Frame\", StringType(), True),\n",
    "    StructField(\"Location Id\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Lodged Bonds\", StringType(), True),\n",
    "    StructField(\"Active Bonds\", StringType(), True),\n",
    "    StructField(\"Closed Bonds\", StringType(), True),\n",
    "    StructField(\"Median Rent\", StringType(), True),\n",
    "    StructField(\"Geometric Mean Rent\", StringType(), True),\n",
    "    StructField(\"Upper Quartile Rent\", StringType(), True),\n",
    "    StructField(\"Lower Quartile Rent\", StringType(), True),\n",
    "    StructField(\"Log Std Dev Weekly Rent\", StringType(), True)\n",
    "])\n",
    "\n",
    "try:\n",
    "    # EXTRACTION PHASE - Check if the file exists\n",
    "    if not os.path.exists(fpath_tenancy_data):\n",
    "        print(f\"Error: File not found at {fpath_tenancy_data}\")\n",
    "    else:\n",
    "        print(f\"Attempting to read data from local file: {fpath_tenancy_data}\")\n",
    "        # EXTRACT file into a Spark DataFrame using the defined schema\n",
    "        df_spark = spark.read.csv(\n",
    "            f\"file://{fpath_tenancy_data}\",\n",
    "            header=True,\n",
    "            schema=schema,  # Use the explicitly defined schema\n",
    "            quote='\"',      # Specify that double quotes are used for quoting\n",
    "            escape='\"'      # Specify that double quotes are used for escaping\n",
    "        )\n",
    "\n",
    "# Review the EXTRACTED DATA\n",
    "        print(\"DataFrame head:\")\n",
    "        df_spark.show()\n",
    "        print(\"DataFrame schema:\")\n",
    "        df_spark.printSchema()\n",
    "\n",
    "        # TRANSFORMATIONS\n",
    "        df_spark_tfm = etl_processor.fn_transform_cast(df_spark, [\"Time Frame\"], \"date\")\n",
    "        df_spark_tfm = etl_processor.fn_transform_cast(df_spark_tfm,\n",
    "         [\"Location Id\", \"Lodged Bonds\", \"Active Bonds\", \"Closed Bonds\",\n",
    "          \"Median Rent\", \"Upper Quartile Rent\", \"Lower Quartile Rent\",\n",
    "          \"Geometric Mean Rent\"], \"integer\")\n",
    "        df_spark_tfm = etl_processor.fn_transform_cast(df_spark_tfm, [\"Log Std Dev Weekly Rent\"], \"double\")\n",
    "\n",
    "        # Select both transformed and orginal fields to perform data quality reports on\n",
    "        selected_fields = [\"Time Frame\", \"tfm_Time Frame\",\n",
    "                         \"Location Id\", \"tfm_Location Id\",\n",
    "                         \"Location\",\n",
    "                         \"Lodged Bonds\", \"tfm_Lodged Bonds\",\n",
    "                         \"Active Bonds\", \"tfm_Active Bonds\",\n",
    "                         \"Closed Bonds\", \"tfm_Closed Bonds\",\n",
    "                         \"Median Rent\", \"tfm_Median Rent\",\n",
    "                         \"Geometric Mean Rent\", \"tfm_Geometric Mean Rent\",\n",
    "                         \"Upper Quartile Rent\", \"tfm_Upper Quartile Rent\",\n",
    "                         \"Lower Quartile Rent\", \"tfm_Lower Quartile Rent\",\n",
    "                         \"Log Std Dev Weekly Rent\", \"tfm_Log Std Dev Weekly Rent\"]\n",
    "        df_spark_tfm = etl_processor.fn_dataframe_selections(df_spark_tfm, selected_fields)\n",
    "\n",
    "        # - DATA QUALITY CHECK REPORT\n",
    "        # - Convert 'Time Frame' column to datetime objects in pandas\n",
    "        df_pandas = df_spark_tfm.toPandas()\n",
    "        df_pandas['tfm_Time Frame'] = pd.to_datetime(df_pandas['tfm_Time Frame'])\n",
    "        ##report = ProfileReport(df_pandas, title=\"Profiling pyspark DataFrame\")\n",
    "        # - Get the current date and time then save the Data Quality Report to github to eyeball (review manually)\n",
    "        ##now = datetime.datetime.now()\n",
    "        ##report.to_file(os.path.join(fpath_data_quality_profile, now.strftime(\"DataProfile_%Y%m%d_%H%M.html\")))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Spark processing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d91ec-47ca-4f16-90c5-e31e0320e558",
   "metadata": {},
   "source": [
    "# 03 - Intermediate ETL of non-geospatial data structures\n",
    "- Dim tables for DATETIME (dim_period including new date/time attributes) and location (dim_location) created\n",
    "- Fact table (fact_tenancy) is created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7d807b-c88b-4d50-8560-9611e35ddae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dim_location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|tfm_Location Id|            Location|\n",
      "+---------------+--------------------+\n",
      "|            -99|                 ALL|\n",
      "|             -1|                  NA|\n",
      "|              1|    Northland Region|\n",
      "|              2|     Auckland Region|\n",
      "|              3|      Waikato Region|\n",
      "|              4|Bay of Plenty Region|\n",
      "|              5|     Gisborne Region|\n",
      "|              6|  Hawke's Bay Region|\n",
      "|              7|     Taranaki Region|\n",
      "|              8|Manawatu-Wanganui...|\n",
      "|              9|   Wellington Region|\n",
      "|             12|   West Coast Region|\n",
      "|             13|   Canterbury Region|\n",
      "|             14|        Otago Region|\n",
      "|             15|    Southland Region|\n",
      "|             16|       Tasman Region|\n",
      "|             17|       Nelson Region|\n",
      "|             18|  Marlborough Region|\n",
      "+---------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- tfm_Location Id: integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n",
      "\n",
      "df_dim_period:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------------+-----------------+\n",
      "|tfm_Time Frame|Year|Annual Quarter|Financial Quarter|\n",
      "+--------------+----+--------------+-----------------+\n",
      "|    1993-02-01|1993|        1993Q1|           1992Q3|\n",
      "|    1993-03-01|1993|        1993Q1|           1992Q3|\n",
      "|    1993-04-01|1993|        1993Q2|           1992Q4|\n",
      "|    1993-05-01|1993|        1993Q2|           1992Q4|\n",
      "|    1993-06-01|1993|        1993Q2|           1992Q4|\n",
      "|    1993-07-01|1993|        1993Q3|           1993Q1|\n",
      "|    1993-08-01|1993|        1993Q3|           1993Q1|\n",
      "|    1993-09-01|1993|        1993Q3|           1993Q1|\n",
      "|    1993-10-01|1993|        1993Q4|           1993Q2|\n",
      "|    1993-11-01|1993|        1993Q4|           1993Q2|\n",
      "|    1993-12-01|1993|        1993Q4|           1993Q2|\n",
      "|    1994-01-01|1994|        1994Q1|           1993Q3|\n",
      "|    1994-02-01|1994|        1994Q1|           1993Q3|\n",
      "|    1994-03-01|1994|        1994Q1|           1993Q3|\n",
      "|    1994-04-01|1994|        1994Q2|           1993Q4|\n",
      "|    1994-05-01|1994|        1994Q2|           1993Q4|\n",
      "|    1994-06-01|1994|        1994Q2|           1993Q4|\n",
      "|    1994-07-01|1994|        1994Q3|           1994Q1|\n",
      "|    1994-08-01|1994|        1994Q3|           1994Q1|\n",
      "|    1994-09-01|1994|        1994Q3|           1994Q1|\n",
      "+--------------+----+--------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- tfm_Time Frame: date (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Annual Quarter: string (nullable = true)\n",
      " |-- Financial Quarter: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_facttenancyorder = [\n",
    "    \"tfm_Time Frame\",\n",
    "    \"tfm_Location Id\",\n",
    "    \"tfm_Lodged Bonds\",\n",
    "    \"tfm_Active Bonds\",\n",
    "    \"tfm_Closed Bonds\",\n",
    "    \"tfm_Median Rent\",\n",
    "    \"tfm_Geometric Mean Rent\",\n",
    "    \"tfm_Upper Quartile Rent\",\n",
    "    \"tfm_Lower Quartile Rent\",\n",
    "    \"tfm_Log Std Dev Weekly Rent\"\n",
    "    ]\n",
    "df_fact_tenancy = etl_processor.fn_dataframe_selections(df_spark_tfm, df_facttenancyorder)\n",
    "\n",
    "# Create df_dim_location\n",
    "location_cols = [\"tfm_Location Id\", \"Location\"]\n",
    "df_dim_location = etl_processor.fn_create_dim_table(df_spark_tfm, location_cols)\n",
    "\n",
    "# Show the location dimension\n",
    "print(\"df_dim_location:\")\n",
    "df_dim_location.show()\n",
    "df_dim_location.printSchema()\n",
    "\n",
    "# Create df_dim_period (basic) - original core of distinct time periods\n",
    "period_cols = [\"tfm_Time Frame\"]\n",
    "df_dim_period = etl_processor.fn_create_dim_table(df_spark_tfm, period_cols)\n",
    "df_dim_period = etl_processor.fn_add_period_attributes(df_dim_period, \"tfm_Time Frame\")\n",
    "\n",
    "# Show the period dimension with added attributes\n",
    "print(\"\\ndf_dim_period:\")\n",
    "df_dim_period.show()\n",
    "df_dim_period.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f5f9f9-bee7-47b6-aa95-25867a164929",
   "metadata": {},
   "source": [
    "# 04 - ETL of geospatial data structures\n",
    "- Quality checking and creation of geospatial data structure (df_dim_geospatial)\n",
    "- Joining of geospatial and location based data table (df_dim_locationV2)\n",
    "- Parquet files generated for all star schema tables (reasoning below):\n",
    "    - dim_location, dim_geospatial, dim_period, and fact_tenancy\n",
    "    - useful for time travel\n",
    "    - Supports both current and future/potential architectures (including icebergs, delta tables, AI development, and support for improved performance in application of analysis and data science methodologies)\n",
    "    - high compression and early initial transformations provide for better resource usage (i.e. processing, storage, memory)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b77409-b0eb-418e-992e-0d9134f18ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "from shapely.geometry import shape\n",
    "\n",
    "fpath_data_geospatialdata = '/content/CommerceCommission202505/src_data/statsnz-regional-council-2023-clipped-generalised-SHP'\n",
    "data = []\n",
    "\n",
    "try:\n",
    "    with fiona.open(fpath_data_geospatialdata, 'r') as source:\n",
    "        for feature in source:\n",
    "            attributes = dict(feature['properties'])\n",
    "            geometry = shape(feature['geometry'])\n",
    "\n",
    "            # Option 1: Get centroid (for point-based maps in Power BI)\n",
    "            if geometry.geom_type in ['Point', 'Polygon', 'LineString', 'MultiPolygon', 'MultiLineString', 'MultiPoint']:\n",
    "                 # Check if geometry is valid before calculating centroid\n",
    "                 if geometry.is_valid:\n",
    "                    centroid = geometry.centroid\n",
    "                    attributes['centroid_longitude'] = centroid.x\n",
    "                    attributes['centroid_latitude'] = centroid.y\n",
    "                 else:\n",
    "                    print(f\"Warning: Invalid geometry found for feature {feature.get('id', 'unknown')}. Cannot calculate centroid.\")\n",
    "                    attributes['centroid_longitude'] = None\n",
    "                    attributes['centroid_latitude'] = None\n",
    "            else:\n",
    "                 attributes['centroid_longitude'] = None\n",
    "                 attributes['centroid_latitude'] = None\n",
    "\n",
    "            # Option 2: Convert to WKT (for use with custom Power BI visuals)\n",
    "            if geometry.is_valid:\n",
    "                attributes['geometry_wkt'] = geometry.wkt\n",
    "            else:\n",
    "                attributes['geometry_wkt'] = None # Assign None if geometry is invalid\n",
    "\n",
    "            data.append(attributes)\n",
    "\n",
    "    # Create Spark DataFrame and manipulate data\n",
    "    df_dim_geospatial = spark.createDataFrame(data)\n",
    "    df_dim_geospatial = fn_transform_cast(df_dim_geospatial, [\"REGC2023_V\"], \"integer\")\n",
    "    geospatial_cols = [\"tfm_REGC2023_V\", \"centroid_longitude\",  \"centroid_latitude\"]\n",
    "    df_dim_geospatial = fn_dataframe_selections(df_dim_geospatial, geospatial_cols)\n",
    "\n",
    "    # Join Geospatial data to Location data\n",
    "    df_dim_locationV2 = df_dim_location.join(df_dim_geospatial, col(\"tfm_Location Id\") == col(\"tfm_REGC2023_V\"), \"left\")\n",
    "    location_cols = [\"tfm_Location Id\", \"Location\", \"centroid_longitude\",  \"centroid_latitude\"]\n",
    "    df_dim_locationV2 = fn_dataframe_selections(df_dim_locationV2, location_cols).orderBy(\"tfm_Location Id\")\n",
    "\n",
    "    # Review datasets\n",
    "    df_dim_locationV2.show()\n",
    "    df_dim_locationV2.printSchema()\n",
    "\n",
    "    # Save to parquet files\n",
    "    df_fact_tenancy.write.parquet(os.path.join(fpath_data_star_schema, \"fact_tenancy.parquet\"), mode=\"overwrite\")\n",
    "    df_dim_locationV2.write.parquet(os.path.join(fpath_data_star_schema,\"dim_location.parquet\"), mode=\"overwrite\")\n",
    "    df_dim_period.write.parquet(os.path.join(fpath_data_star_schema, \"dim_period.parquet\"), mode=\"overwrite\")\n",
    "    df_dim_geospatial.write.parquet(os.path.join(fpath_data_star_schema, \"dim_geospatial.parquet\"), mode=\"overwrite\")\n",
    "\n",
    "except fiona.errors.DriverError as e:\n",
    "    print(f\"Error opening shapefile: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
