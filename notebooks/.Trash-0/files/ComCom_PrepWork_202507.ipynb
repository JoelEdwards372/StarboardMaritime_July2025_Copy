{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd80be39",
   "metadata": {},
   "source": [
    "# 01 - Acquire Packages For Data Preparation Work\n",
    "Includes:\n",
    " - loading required PACKAGES\n",
    " - provided DIRECTORY for filepaths and github/version control functionality for structuring data and frameworks (not fully used as had limited time)\n",
    " - FUNCTIONS (which would have been encapsulated within classes and appropriate github/version control directories - but with limited time only included here).\n",
    " - A data type/preparation function (TRANSFORMATION - fn_transform_cast)\n",
    " - A function which creates dimensional tables (TRANSFORMATION for Star Schemas - fn_create_dim_table)\n",
    " - A function that creates spark dataframes based on user suggestions (EXTRACTION/LOAD - fn_dataframe_selections)\n",
    " - A function that extends/creates attributes relevant to dates (TRANSFORMATION - fn_add_period_attributes)\n",
    " - Functions include testing, error checking, correction, and validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType\n",
    "from pyspark.sql.functions import col, to_date, when, regexp_replace, year, quarter, concat, lit, when, month, ceil\n",
    "from ydata_profiling import ProfileReport\n",
    "import os # Import os module for path manipulation\n",
    "import shutil # Integration with version control platforms (github, gitlab, etc.)\n",
    "import pandas as pd # Import pandas\n",
    "import datetime\n",
    "from etl_development.joels_etl_class import JoelsETL\n",
    "\n",
    "# Pathways to (1) data, \n",
    "# (2) output data quality/profiling used to examine and determine transformation requirements, \n",
    "# (3) star schema output (warehouse)\n",
    "fpath_tenancy_data = '/app/src_data/'\n",
    "fpath_etl_code = '/app/etl_development/'\n",
    "fpath_data_quality_profile = '/app/data_quality_profiles/'\n",
    "fpath_data_star_schema = '/app/data_star_schema_prep/'\n",
    "\n",
    "\n",
    "# FUNCTIONS (Placed in classes in the ETL development directory - but have retained here in notebook)\n",
    "# THE CLASS CONTAINING FUNCTIONS WOULD HAVE BEEN imported via .... \"from etl_development.etl.joels_etl_class import JoelsETL\"\n",
    "# THEN INSTANTIATED AS A ETL class ... \"joels_etl = JoelsETL(spark)\" ... with functions called via joels_etl.fn_transform_cast etc...\n",
    "\n",
    "# Function that explicitly TRANSFORMS string type fields that are either date, integer, or double type\n",
    "def fn_transform_cast(df, columns, cast_type):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "        Casts multiple string columns to a specified data type in a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input PySpark DataFrame.\n",
    "        columns (list of str): A list of column names to be cast.\n",
    "        cast_type (str): The target data type for casting (\"date\", \"integer\", or \"double\").\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with specified columns cast to the target type.\n",
    "\n",
    "    Use Case:\n",
    "    i.e. fn_transform_cast(df, [\"col1\", \"col2\"], \"integer\")\n",
    "    i.e. fn_transform_cast(df, [\"col3\"], \"date\")\n",
    "    \"\"\"\n",
    "    valid_cast_types = [\"date\", \"integer\", \"double\"]\n",
    "    if cast_type.lower() not in valid_cast_types:\n",
    "        raise ValueError(f\"Invalid cast_type. Supported types are: {valid_cast_types}\")\n",
    "\n",
    "    for column in columns:\n",
    "        new_col_name = f\"tfm_{column}\"\n",
    "        if cast_type.lower() == \"date\":\n",
    "            df = df.withColumn(new_col_name, col(column).cast(\"date\"))\n",
    "        else:\n",
    "            # For integer and double, remove commas before casting\n",
    "            df = df.withColumn(new_col_name, regexp_replace(col(column), \",\", \"\").cast(cast_type))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to TRANSFORM tables into dimensional tables\n",
    "def fn_create_dim_table(df_input, dimension_cols, order_by_col=None):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "        Creates a dimension table from a Spark DataFrame based on specified columns.\n",
    "\n",
    "    Args:\n",
    "        df_input (DataFrame): The input PySpark DataFrame with transformed columns.\n",
    "        dimension_cols (list of str): A list of column names to be included\n",
    "                                     in the dimension table.\n",
    "        order_by_col (str, optional): The name of the column to order the dimension\n",
    "                                      table by. If None, the first column in\n",
    "                                      dimension_cols is used for ordering.\n",
    "                                      Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The created PySpark dimension table.\n",
    "\n",
    "    Use Case (Creating Location Dimension):\n",
    "    df_dim_location = create_dimension_table(\n",
    "        df_spark_tfm,\n",
    "        [\"tfm_Location Id\", \"Location\"],\n",
    "        \"tfm_Location Id\" # Optional: explicitly specify the order column\n",
    "    )\n",
    "\n",
    "    Use Case (Creating Period Dimension - basic):\n",
    "    df_dim_period_basic = create_dimension_table(\n",
    "        df_spark_tfm,\n",
    "        [\"tfm_Time Frame\"]\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Validate input columns\n",
    "    if not isinstance(dimension_cols, list) or not dimension_cols:\n",
    "        raise ValueError(\"dimension_cols must be a non-empty list of column names.\")\n",
    "\n",
    "    if not all(c in df_input.columns for c in dimension_cols):\n",
    "        missing_cols = [c for c in dimension_cols if c not in df_input.columns]\n",
    "        raise ValueError(f\"Input DataFrame is missing required columns: {missing_cols}\")\n",
    "\n",
    "    # Determine the column for ordering\n",
    "    if order_by_col is None:\n",
    "        order_col = dimension_cols[0]\n",
    "    else:\n",
    "        if order_by_col not in dimension_cols:\n",
    "             raise ValueError(f\"order_by_col '{order_by_col}' must be one of the columns in dimension_cols.\")\n",
    "        order_col = order_by_col\n",
    "\n",
    "    # Create the dimension table\n",
    "    df_dimension = (df_input.\n",
    "                    select(dimension_cols).\n",
    "                    distinct().\n",
    "                    orderBy(order_col))\n",
    "\n",
    "    return df_dimension\n",
    "\n",
    "\n",
    "# Function that creates new dataframes, accepting user field selections\n",
    "def fn_dataframe_selections(df_spark, selected_fields):\n",
    "  \"\"\"\n",
    "  Selects and reorders a subset of columns in a Spark DataFrame\n",
    "  according to a provided list.\n",
    "\n",
    "  Args:\n",
    "    df_spark (DataFrame): The input PySpark DataFrame.\n",
    "    selected_fields (list of str): A list of column names\n",
    "                                          specifying the subset of columns\n",
    "                                          to select and their desired order.\n",
    "                                          These column names should match\n",
    "                                          names present in the input DataFrame.\n",
    "\n",
    "  Returns:\n",
    "    DataFrame: A new DataFrame with the specified columns in the desired order.\n",
    "               Returns None if selected_fields is empty or not a list.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If any column in desired_subset_order is not found\n",
    "                  in the input DataFrame.\n",
    "\n",
    "  Use Cases (including testing):\n",
    "\tExample of how to use the function:\n",
    "\tAssuming df_spark is your Spark DataFrame\n",
    "\n",
    "\tExample 1: Select and reorder a few columns\n",
    "\tselected_fields = [\"tfm_Time Frame\", \"Location\", \"tfm_Median Rent\"]\n",
    "\tdf_subset1 = fn_dataframe_selections(df_spark, selected_fields)\n",
    "\tif df_subset1 is not None:\n",
    "\t\tdf_subset1.show()\n",
    "\n",
    "\tExample 2: Select and reorder almost all columns in a specific order\n",
    "\tselected_fields = [\"tfm_Location Id\", \"Location\", \"tfm_Time Frame\", \"tfm_Lodged Bonds\",\n",
    "\t\"tfm_Active Bonds\", \"tfm_Closed Bonds\", \"tfm_Median Rent\"]\n",
    "\tdf_subset2 = fn_dataframe_selections(df_spark, selected_fields)\n",
    "\tif df_subset2 is not None:\n",
    "\t\tdf_subset2.show()\n",
    "\n",
    "\tExample 3: Handling an invalid column (will raise ValueError)\n",
    "\ttry:\n",
    "\t\tselected_fields = [\"tfm_Time Frame\", \"NonExistentColumn\"]\n",
    "\t\tdf_invalid = fn_dataframe_selections(df_spark, selected_fields)\n",
    "\texcept ValueError as e:\n",
    "\t\tprint(f\"Caught expected error: {e}\")\n",
    "  \"\"\"\n",
    "  if not isinstance(selected_fields, list) or not selected_fields:\n",
    "      print(\"Error: desired_subset_order must be a non-empty list of column names.\")\n",
    "      return None\n",
    "\n",
    "  # Check if all requested columns exist in the DataFrame\n",
    "  missing_cols = [col_name for col_name in selected_fields if col_name not in df_spark.columns]\n",
    "  if missing_cols:\n",
    "      raise ValueError(f\"The following requested columns are not found in the DataFrame: {missing_cols}\")\n",
    "\n",
    "  # Select and reorder the columns\n",
    "  return df_spark.select(selected_fields)\n",
    "\n",
    "\n",
    "# Function that expands of data/time fields for dim date relevant tables\n",
    "def fn_add_period_attributes(df_spark, date_column):\n",
    "  \"\"\"\n",
    "  Adds Year, Annual Quarter, and Financial Quarter attributes to a Spark DataFrame\n",
    "  based on a specified date column.\n",
    "\n",
    "  Args:\n",
    "    df_spark (DataFrame): The input PySpark DataFrame.\n",
    "    date_column (str): The name of the date column from which to calculate the attributes.\n",
    "                       This column should be of a date or timestamp type.\n",
    "\n",
    "  Returns:\n",
    "    DataFrame: A new DataFrame with the added period attributes.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the specified date_column is not found in the DataFrame\n",
    "                or is not of a date/timestamp type.\n",
    "\n",
    "  Use Cases:\n",
    "  # Assuming df_dim_period is your Spark DataFrame and 'tfm_Time Frame' would be the date column\n",
    "  # df_dim_period_with_attributes = add_period_attributes(df_dim_period, \"tfm_Time Frame\")\n",
    "  \"\"\"\n",
    "  if date_column not in df_spark.columns:\n",
    "      raise ValueError(f\"The specified date column '{date_column}' is not found in the DataFrame.\")\n",
    "\n",
    "  # Check if the column is a date or timestamp type\n",
    "  column_type = df_spark.schema[date_column].dataType\n",
    "  if not (isinstance(column_type, DateType) or isinstance(column_type, TimestampType)):\n",
    "       raise ValueError(f\"The column '{date_column}' is not a date or timestamp type. \"\n",
    "                        f\"Current type is {column_type}.\")\n",
    "\n",
    "  df_with_attributes = (df_spark.\n",
    "                 withColumn(\"Year\", year(col(date_column))).\n",
    "                 withColumn(\"Annual Quarter\", concat(year(col(date_column)), lit(\"Q\"), quarter(col(date_column)))).\n",
    "                 withColumn(\"Financial Quarter\",\n",
    "                            concat(\n",
    "                                when(month(col(date_column)) >= 7, year(col(date_column)))\n",
    "                                .otherwise(year(col(date_column)) - 1),\n",
    "                                lit(\"Q\"),\n",
    "                                when(month(col(date_column)) >= 7, ceil((month(col(date_column)) - 6) / 3))\n",
    "                                .otherwise(ceil((month(col(date_column)) + 6) / 3))\n",
    "                            )\n",
    "                 ))\n",
    "  return df_with_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28b948",
   "metadata": {},
   "source": [
    "# 02 - Setup Spark to ingest and ETL non-geospatial data\n",
    "Includes:\n",
    "- creating a spark session\n",
    "- accessing tenancy data from original file with ...\n",
    "- a pre-formed schema (non-parsing) to EXTRACT data (as is)\n",
    "- basic TRANSFORMATION/PREPARATION of non-geospatial datasets\n",
    "- examination of data quality leveraging YData Quality Framework tool (and saving to \\\"data quality profile\\\" folder in interactive html format.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadCSVLocal\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Define the schema explicitly\n",
    "# All columns are declared as StringType to load all and minimise parsing errors\n",
    "# due to mixed types or special values like \"-1\" or \"NA\" within quoted fields.\n",
    "# Casting occurs later\n",
    "\"\"\"\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\")  # Set to \"true\" if your CSV has a header row\n",
    "    .option(\"inferSchema\", \"false\") # Crucial: This makes all columns StringType\n",
    "    .csv(csv_file_path)\n",
    "\"\"\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Time Frame\", StringType(), True),\n",
    "    StructField(\"Location Id\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Lodged Bonds\", StringType(), True),\n",
    "    StructField(\"Active Bonds\", StringType(), True),\n",
    "    StructField(\"Closed Bonds\", StringType(), True),\n",
    "    StructField(\"Median Rent\", StringType(), True),\n",
    "    StructField(\"Geometric Mean Rent\", StringType(), True),\n",
    "    StructField(\"Upper Quartile Rent\", StringType(), True),\n",
    "    StructField(\"Lower Quartile Rent\", StringType(), True),\n",
    "    StructField(\"Log Std Dev Weekly Rent\", StringType(), True)\n",
    "])\n",
    "\n",
    "try:\n",
    "    # EXTRACTION PHASE - Check if the file exists\n",
    "    if not os.path.exists(fpath_tenancy_data):\n",
    "        print(f\"Error: File not found at {fpath_tenancy_data}\")\n",
    "    else:\n",
    "        print(f\"Attempting to read data from local file: {fpath_tenancy_data}\")\n",
    "        # EXTRACT file into a Spark DataFrame using the defined schema\n",
    "        df_spark = spark.read.csv(\n",
    "            f\"file://{fpath_tenancy_data}\",\n",
    "            header=True,\n",
    "            schema=schema,  # Use the explicitly defined schema\n",
    "            quote='\"',      # Specify that double quotes are used for quoting\n",
    "            escape='\"'      # Specify that double quotes are used for escaping\n",
    "        )\n",
    "\n",
    "# Review the EXTRACTED DATA\n",
    "        print(\"DataFrame head:\")\n",
    "        df_spark.show()\n",
    "        print(\"DataFrame schema:\")\n",
    "        df_spark.printSchema()\n",
    "\n",
    "        # TRANSFORMATIONS\n",
    "        df_spark_tfm = fn_transform_cast(df_spark, [\"Time Frame\"], \"date\")\n",
    "        df_spark_tfm = fn_transform_cast(df_spark_tfm,\n",
    "         [\"Location Id\", \"Lodged Bonds\", \"Active Bonds\", \"Closed Bonds\",\n",
    "          \"Median Rent\", \"Upper Quartile Rent\", \"Lower Quartile Rent\",\n",
    "          \"Geometric Mean Rent\"], \"integer\")\n",
    "        df_spark_tfm = fn_transform_cast(df_spark_tfm, [\"Log Std Dev Weekly Rent\"], \"double\")\n",
    "\n",
    "        # Select both transformed and orginal fields to perform data quality reports on\n",
    "        selected_fields = [\"Time Frame\", \"tfm_Time Frame\",\n",
    "                         \"Location Id\", \"tfm_Location Id\",\n",
    "                         \"Location\",\n",
    "                         \"Lodged Bonds\", \"tfm_Lodged Bonds\",\n",
    "                         \"Active Bonds\", \"tfm_Active Bonds\",\n",
    "                         \"Closed Bonds\", \"tfm_Closed Bonds\",\n",
    "                         \"Median Rent\", \"tfm_Median Rent\",\n",
    "                         \"Geometric Mean Rent\", \"tfm_Geometric Mean Rent\",\n",
    "                         \"Upper Quartile Rent\", \"tfm_Upper Quartile Rent\",\n",
    "                         \"Lower Quartile Rent\", \"tfm_Lower Quartile Rent\",\n",
    "                         \"Log Std Dev Weekly Rent\", \"tfm_Log Std Dev Weekly Rent\"]\n",
    "        df_spark_tfm = fn_dataframe_selections(df_spark_tfm, selected_fields)\n",
    "\n",
    "        # - DATA QUALITY CHECK REPORT\n",
    "        # - Convert 'Time Frame' column to datetime objects in pandas\n",
    "        df_pandas = df_spark_tfm.toPandas()\n",
    "        df_pandas['tfm_Time Frame'] = pd.to_datetime(df_pandas['tfm_Time Frame'])\n",
    "        report = ProfileReport(df_pandas, title=\"Profiling pyspark DataFrame\")\n",
    "        # - Get the current date and time then save the Data Quality Report to github to eyeball (review manually)\n",
    "        now = datetime.datetime.now()\n",
    "        report.to_file(os.path.join(fpath_data_quality_profile, now.strftime(\"DataProfile_%Y%m%d_%H%M.html\")))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Spark processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1498ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
